{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e46621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory created: ../outputs/figures/modeling\n",
      "Models directory created: ../outputs/models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 03_modeling.ipynb\n",
    "# Model Training and Evaluation for Multi-Class Diabetes Classification\n",
    "# Dataset: BRFSS 2015 - Diabetes Health Indicators (3 Classes)\n",
    "# \n",
    "# OPTIMIZATION GOAL: HIGH RECALL\n",
    "# Medical Context: In diabetes screening, it is more important to identify\n",
    "# all potential diabetes cases (high recall) even if it means some false \n",
    "# positives. Missing a diabetes case (false negative) has more serious \n",
    "# health consequences than a false alarm (false positive).\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path to import custom modules\n",
    "sys.path.append('../src/core')\n",
    "\n",
    "# Import custom modules for feature engineering and modeling\n",
    "from feature_engineering import apply_all_feature_engineering\n",
    "from modeling import (\n",
    "    train_logistic_regression,\n",
    "    train_random_forest,\n",
    "    train_xgboost,\n",
    "    train_linear_svm,\n",
    "    evaluate_model,\n",
    "    plot_confusion_matrix,\n",
    "    plot_classification_report,\n",
    "    plot_roc_curves,\n",
    "    compare_models,\n",
    "    save_model\n",
    ")\n",
    "\n",
    "# Scikit-learn imports for scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configure plot style for consistent visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Define output directories for saving visualizations and models\n",
    "output_dir = \"../outputs/figures/modeling\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output directory created: {output_dir}\")\n",
    "\n",
    "models_dir = \"../outputs/models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"Models directory created: {models_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbe2418",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d84368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LOAD PREPROCESSED DATA\n",
      "================================================================================\n",
      "\n",
      "DATA SHAPES:\n",
      "------------------------------------------------------------\n",
      "Training Set (Original):     (183824, 21)\n",
      "Training Set (SMOTE):        (456129, 21)\n",
      "Test Set:                    (45957, 21)\n",
      "\n",
      "TARGET DISTRIBUTION:\n",
      "------------------------------------------------------------\n",
      "Training Set (Original):\n",
      "Diabetes_012\n",
      "0.0    152043\n",
      "1.0      3703\n",
      "2.0     28078\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training Set (SMOTE):\n",
      "Diabetes_012\n",
      "0.0    152043\n",
      "1.0    152043\n",
      "2.0    152043\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Set:\n",
      "Diabetes_012\n",
      "0.0    38012\n",
      "1.0      926\n",
      "2.0     7019\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. LOAD PREPROCESSED DATA\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LOAD PREPROCESSED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load scaled training and test data\n",
    "# These datasets have continuous features scaled (StandardScaler)\n",
    "# Binary and ordinal features remain unchanged\n",
    "features_train = pd.read_csv(\"../data/processed/features_train_scaled.csv\")\n",
    "features_test = pd.read_csv(\"../data/processed/features_test_scaled.csv\")\n",
    "\n",
    "# Load target variables\n",
    "# squeeze() converts single-column DataFrame to Series\n",
    "target_train = pd.read_csv(\"../data/processed/target_train.csv\").squeeze()\n",
    "target_test = pd.read_csv(\"../data/processed/target_test.csv\").squeeze()\n",
    "\n",
    "# Load SMOTE-resampled training data\n",
    "# SMOTE (Synthetic Minority Over-sampling Technique) creates synthetic samples\n",
    "# for minority classes to balance the dataset\n",
    "features_train_smote = pd.read_csv(\"../data/processed/features_train_smote.csv\")\n",
    "target_train_smote = pd.read_csv(\"../data/processed/target_train_smote.csv\").squeeze()\n",
    "\n",
    "# Display data shapes to verify successful loading\n",
    "print(\"\\nDATA SHAPES:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training Set (Original):     {features_train.shape}\")\n",
    "print(f\"Training Set (SMOTE):        {features_train_smote.shape}\")\n",
    "print(f\"Test Set:                    {features_test.shape}\")\n",
    "\n",
    "# Display target distribution to understand class imbalance\n",
    "# Class 0 = No Diabetes\n",
    "# Class 1 = Prediabetes\n",
    "# Class 2 = Diabetes\n",
    "print(\"\\nTARGET DISTRIBUTION:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Training Set (Original):\")\n",
    "print(target_train.value_counts().sort_index()) # pyright: ignore[reportAttributeAccessIssue]\n",
    "print(\"\\nTraining Set (SMOTE):\")\n",
    "print(target_train_smote.value_counts().sort_index()) # pyright: ignore[reportAttributeAccessIssue]\n",
    "print(\"\\nTest Set:\")\n",
    "print(target_test.value_counts().sort_index()) # pyright: ignore[reportAttributeAccessIssue]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4dfbf",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45d0cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "APPLYING FEATURE ENGINEERING:\n",
      "------------------------------------------------------------\n",
      "Creating new features:\n",
      "  • HealthRiskScore: Sum of risk factors (HighBP, HighChol, Stroke, etc.)\n",
      "  • LifestyleScore: Sum of positive health habits (PhysActivity, Fruits, Veggies)\n",
      "  • BMI Categories: One-hot encoded BMI groups (Underweight, Normal, Overweight, Obese)\n",
      "  • Age Groups: One-hot encoded age groups (Young, Middle, Senior)\n",
      "  • Interaction Features: Product of related features (BMI×HighBP, Age×BMI, GenHlth×PhysActivity)\n",
      "Feature engineering completed. New shape: (183824, 33)\n",
      "Feature engineering completed. New shape: (45957, 33)\n",
      "Feature engineering completed. New shape: (456129, 33)\n",
      "\n",
      "Feature engineering completed\n",
      "Original features:  21\n",
      "Engineered features: 33\n",
      "New features added:  12\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nAPPLYING FEATURE ENGINEERING:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Creating new features:\")\n",
    "print(\"  • HealthRiskScore: Sum of risk factors (HighBP, HighChol, Stroke, etc.)\")\n",
    "print(\"  • LifestyleScore: Sum of positive health habits (PhysActivity, Fruits, Veggies)\")\n",
    "print(\"  • BMI Categories: One-hot encoded BMI groups (Underweight, Normal, Overweight, Obese)\")\n",
    "print(\"  • Age Groups: One-hot encoded age groups (Young, Middle, Senior)\")\n",
    "print(\"  • Interaction Features: Product of related features (BMI×HighBP, Age×BMI, GenHlth×PhysActivity)\")\n",
    "\n",
    "# Apply feature engineering to all three datasets\n",
    "# This creates composite features that may have better predictive power\n",
    "# than individual features alone\n",
    "features_train_eng = apply_all_feature_engineering(features_train)\n",
    "features_test_eng = apply_all_feature_engineering(features_test)\n",
    "features_train_smote_eng = apply_all_feature_engineering(features_train_smote)\n",
    "\n",
    "# Display feature engineering results\n",
    "print(\"\\nFeature engineering completed\")\n",
    "print(f\"Original features:  {features_train.shape[1]}\")\n",
    "print(f\"Engineered features: {features_train_eng.shape[1]}\")\n",
    "print(f\"New features added:  {features_train_eng.shape[1] - features_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3648951",
   "metadata": {},
   "source": [
    "# Scale New Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19aa4ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: SCALE NEW CONTINUOUS FEATURES\n",
      "================================================================================\n",
      "\n",
      "SCALING NEW CONTINUOUS FEATURES:\n",
      "------------------------------------------------------------\n",
      "Features to scale: ['HealthRiskScore', 'LifestyleScore', 'BMI_x_HighBP', 'Age_x_BMI', 'GenHlth_x_PhysActivity']\n",
      "\n",
      "New continuous features scaled\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. SCALE NEW CONTINUOUS FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: SCALE NEW CONTINUOUS FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify new continuous features that need scaling\n",
    "# These are the newly created features that have continuous values\n",
    "# Scaling ensures all features have similar ranges (mean=0, std=1)\n",
    "new_continuous_features = ['HealthRiskScore', 'LifestyleScore', 'BMI_x_HighBP', \n",
    "                           'Age_x_BMI', 'GenHlth_x_PhysActivity']\n",
    "\n",
    "print(\"\\nSCALING NEW CONTINUOUS FEATURES:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Features to scale: {new_continuous_features}\")\n",
    "\n",
    "# Initialize StandardScaler for new features\n",
    "# StandardScaler: (x - mean) / std\n",
    "scaler_new = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data and transform training set\n",
    "# fit_transform() calculates mean and std from training data\n",
    "features_train_eng[new_continuous_features] = scaler_new.fit_transform(\n",
    "    features_train_eng[new_continuous_features]\n",
    ")\n",
    "\n",
    "# Transform test set using training statistics\n",
    "# Important: Use transform() only (not fit_transform()) to avoid data leakage\n",
    "features_test_eng[new_continuous_features] = scaler_new.transform(\n",
    "    features_test_eng[new_continuous_features]\n",
    ")\n",
    "\n",
    "# Transform SMOTE training set using same statistics\n",
    "# This ensures consistency across all datasets\n",
    "features_train_smote_eng[new_continuous_features] = scaler_new.transform(\n",
    "    features_train_smote_eng[new_continuous_features]\n",
    ")\n",
    "\n",
    "print(\"\\nNew continuous features scaled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8b2e5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a48355ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: MODEL TRAINING - BASELINE (WITHOUT SMOTE)\n",
      "OPTIMIZATION: HIGH RECALL FOR MEDICAL SCREENING\n",
      "================================================================================\n",
      "\n",
      "MEDICAL CONTEXT:\n",
      "------------------------------------------------------------\n",
      "In diabetes screening, HIGH RECALL is critical because:\n",
      "  • Missing a diabetes case (False Negative) can lead to serious health complications\n",
      "  • False alarms (False Positives) can be verified with follow-up tests\n",
      "  • Early detection saves lives and reduces treatment costs\n",
      "  • It's better to screen more people than to miss actual cases\n",
      "\n",
      "TRAINING BASELINE MODELS (RECALL-OPTIMIZED):\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1/4] Training Logistic Regression (Recall-Optimized)...\n",
      "      • Using class_weight='balanced' to prioritize minority classes\n",
      "      • Linear model with L2 regularization (C=1.0)\n",
      "      • Max iterations: 1000 for convergence\n",
      "      Recall: 0.6279\n",
      "\n",
      "[2/4] Training Random Forest (Recall-Optimized)...\n",
      "      • Using class_weight='balanced' to prioritize minority classes\n",
      "      • Ensemble of 100 decision trees\n",
      "      • Max depth: 10 to prevent overfitting\n",
      "      • Random state: 42 for reproducibility\n",
      "      Recall: 0.6700\n",
      "\n",
      "[3/4] Training XGBoost (Recall-Optimized)...\n",
      "      • Using scale_pos_weight to balance classes\n",
      "      • Gradient boosting with 100 estimators\n",
      "      • Learning rate: 0.1\n",
      "      • Max depth: 5\n",
      "      • Optimized for multi-class classification\n",
      "      Recall: 0.8375\n",
      "\n",
      "[4/4] Training LinearSVM (Recall-Optimized) - ON 10% SAMPLE...\n",
      "      LinearSVM trained on 10% sample for computational efficiency\n",
      "      • Full dataset training would take significant time\n",
      "      • Sample training provides performance estimate\n",
      "      • Using class_weight='balanced' to prioritize minority classes\n",
      "      • Linear kernel (much faster than RBF)\n",
      "      • Calibrated for probability estimates\n",
      "      • Training on 18,382 samples (10% of 183,824)\n",
      "      Recall: 0.8354\n",
      "      Note: Results based on 10% sample, not full training data\n",
      "\n",
      "Baseline models trained successfully\n",
      "All models optimized for HIGH RECALL to minimize false negatives\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. MODEL TRAINING - BASELINE (WITHOUT SMOTE) - OPTIMIZED FOR RECALL\n",
    "# =============================================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: MODEL TRAINING - BASELINE (WITHOUT SMOTE)\")\n",
    "print(\"OPTIMIZATION: HIGH RECALL FOR MEDICAL SCREENING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nMEDICAL CONTEXT:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"In diabetes screening, HIGH RECALL is critical because:\")\n",
    "print(\"  • Missing a diabetes case (False Negative) can lead to serious health complications\")\n",
    "print(\"  • False alarms (False Positives) can be verified with follow-up tests\")\n",
    "print(\"  • Early detection saves lives and reduces treatment costs\")\n",
    "print(\"  • It's better to screen more people than to miss actual cases\")\n",
    "\n",
    "print(\"\\nTRAINING BASELINE MODELS (RECALL-OPTIMIZED):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Initialize dictionaries to store models and results\n",
    "models_baseline = {}\n",
    "results_baseline = {}\n",
    "\n",
    "# 4.1 Logistic Regression\n",
    "print(\"\\n[1/4] Training Logistic Regression (Recall-Optimized)...\")\n",
    "print(\"      • Using class_weight='balanced' to prioritize minority classes\")\n",
    "print(\"      • Linear model with L2 regularization (C=1.0)\")\n",
    "print(\"      • Max iterations: 1000 for convergence\")\n",
    "lr_model = train_logistic_regression(features_train_eng, target_train) # pyright: ignore[reportArgumentType]\n",
    "models_baseline['Logistic Regression'] = lr_model\n",
    "results_baseline['Logistic Regression'] = evaluate_model(\n",
    "    lr_model, features_test_eng, target_test # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "print(f\"      Recall: {results_baseline['Logistic Regression']['Recall']:.4f}\")\n",
    "\n",
    "# 4.2 Random Forest\n",
    "print(\"\\n[2/4] Training Random Forest (Recall-Optimized)...\")\n",
    "print(\"      • Using class_weight='balanced' to prioritize minority classes\")\n",
    "print(\"      • Ensemble of 100 decision trees\")\n",
    "print(\"      • Max depth: 10 to prevent overfitting\")\n",
    "print(\"      • Random state: 42 for reproducibility\")\n",
    "rf_model = train_random_forest(features_train_eng, target_train) # pyright: ignore[reportArgumentType]\n",
    "models_baseline['Random Forest'] = rf_model\n",
    "results_baseline['Random Forest'] = evaluate_model(\n",
    "    rf_model, features_test_eng, target_test # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "print(f\"      Recall: {results_baseline['Random Forest']['Recall']:.4f}\")\n",
    "\n",
    "# 4.3 XGBoost\n",
    "print(\"\\n[3/4] Training XGBoost (Recall-Optimized)...\")\n",
    "print(\"      • Using scale_pos_weight to balance classes\")\n",
    "print(\"      • Gradient boosting with 100 estimators\")\n",
    "print(\"      • Learning rate: 0.1\")\n",
    "print(\"      • Max depth: 5\")\n",
    "print(\"      • Optimized for multi-class classification\")\n",
    "xgb_model = train_xgboost(features_train_eng, target_train) # pyright: ignore[reportArgumentType]\n",
    "models_baseline['XGBoost'] = xgb_model\n",
    "results_baseline['XGBoost'] = evaluate_model(\n",
    "    xgb_model, features_test_eng, target_test # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "print(f\"      Recall: {results_baseline['XGBoost']['Recall']:.4f}\")\n",
    "\n",
    "# 4.4 LinearSVM (on 10% sample for speed)\n",
    "print(\"\\n[4/4] Training LinearSVM (Recall-Optimized) - ON 10% SAMPLE...\")\n",
    "print(\"      LinearSVM trained on 10% sample for computational efficiency\")\n",
    "print(\"      • Full dataset training would take significant time\")\n",
    "print(\"      • Sample training provides performance estimate\")\n",
    "print(\"      • Using class_weight='balanced' to prioritize minority classes\")\n",
    "print(\"      • Linear kernel (much faster than RBF)\")\n",
    "print(\"      • Calibrated for probability estimates\")\n",
    "\n",
    "# Sample 10% of training data for LinearSVM\n",
    "features_train_sample, _, target_train_sample, _ = train_test_split(\n",
    "    features_train_eng, target_train, \n",
    "    train_size=0.1, \n",
    "    random_state=42, \n",
    "    stratify=target_train # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "\n",
    "print(f\"      • Training on {len(features_train_sample):,} samples (10% of {len(features_train_eng):,})\")\n",
    "\n",
    "svm_model = train_linear_svm(features_train_sample, target_train_sample) \n",
    "models_baseline['LinearSVM (10%)'] = svm_model\n",
    "results_baseline['LinearSVM (10%)'] = evaluate_model(\n",
    "    svm_model, features_test_eng, target_test # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "print(f\"      Recall: {results_baseline['LinearSVM (10%)']['Recall']:.4f}\")\n",
    "print(\"      Note: Results based on 10% sample, not full training data\")\n",
    "\n",
    "print(\"\\nBaseline models trained successfully\")\n",
    "print(\"All models optimized for HIGH RECALL to minimize false negatives\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpp-template (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
